{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE, K = 3, 100\n",
    "VOCAB_SIZE, IN_EMBED_SIZE, OUT_ENBED_SIZE = 30000, 100, 100 \n",
    "BATCH_SIZE = 128\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "\n",
    "vocab_file = 'text8'\n",
    "#prepare vocabulary\n",
    "def tokenize(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "text = None\n",
    "with open(vocab_file) as fr:\n",
    "    text = fr.readlines()[0]\n",
    "    token_list = tokenize(text)\n",
    "vocab =  Counter(token_list).most_common(VOCAB_SIZE - 1)\n",
    "idx_to_word = [item[0] for item in vocab]\n",
    "idx_to_word.append('UNK')\n",
    "word_to_idx = {item: i for i, item in enumerate(idx_to_word)}\n",
    "word_counts = [item[1] for item in vocab]\n",
    "word_counts.append(len(text) - np.sum(word_counts))\n",
    "frequence = word_counts / np.sum(word_counts)\n",
    "frequence = frequence ** (3 / 4)\n",
    "frequence = frequence / np.sum(frequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "class MyDataset(tud.Dataset):\n",
    "    def __init__(self, text, idx_to_word, word_to_idx, WINDOW_SIZE, K, device, frequence):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.text = text\n",
    "        self.idx_to_word = idx_to_word\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.window_size = WINDOW_SIZE\n",
    "        self.k = K\n",
    "        self.device = device\n",
    "        self.frequence = torch.FloatTensor(frequence)\n",
    "        self.word_encode = torch.LongTensor([self.word_to_idx.get(word, self.word_to_idx['UNK']) \\\n",
    "                                             for word in self.text])\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.word_encode)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        center_word = self.word_encode[idx]\n",
    "        pos_index = list(range(idx - self.window_size, idx)) + list(range(idx + 1, idx + self.window_size + 1))\n",
    "        pos_index = [idx % len(self.word_encode) for idx in pos_index]\n",
    "        pos_words = self.word_encode[pos_index]\n",
    "        neg_words = torch.multinomial(self.frequence, self.k * self.window_size * 2, replacement = True)\n",
    "        return center_word, pos_words, neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0\n",
      "torch.Size([128])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 600])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataset = MyDataset(text, idx_to_word, word_to_idx, WINDOW_SIZE, K, device, frequence)  \n",
    "dataloader = tud.DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = True)  \n",
    "for i, (center_word, pos_words, neg_words) in enumerate(dataloader):\n",
    "    print(\"iter: {}\".format(i))\n",
    "    print(center_word.shape)\n",
    "    print(pos_words.shape)\n",
    "    print(neg_words.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, VOCAB_SIZE, IN_EMBED_SIZE, OUT_ENBED_SIZE):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.in_embedding = nn.Embedding(VOCAB_SIZE, IN_EMBED_SIZE)\n",
    "        initrange = 0.5 / IN_EMBED_SIZE\n",
    "        self.in_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out_embedding = nn.Embedding(VOCAB_SIZE, OUT_ENBED_SIZE)\n",
    "        self.out_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, center_word, pos_words, neg_words):\n",
    "        center_embeddings = self.in_embedding(center_word)  #[batch-size, embed_size]\n",
    "        pos_embeddings = self.in_embedding(pos_words)      #[batch-size, window_size * 2, embed_size]\n",
    "        neg_embeddings = self.out_embedding(neg_words)      #[batch-size, window_size * 2 * K, embed_size]\n",
    "        center_embeddings = center_embeddings.unsqueeze(2)                     #[batch-size, embed_size, 1]\n",
    "        \n",
    "        pos_dot = pos_embeddings\n",
    "        \n",
    "        pos_dot = torch.bmm(pos_embeddings, center_embeddings).squeeze(2)  #[batch-size, window_size * 2]\n",
    "        neg_dot = torch.bmm(neg_embeddings, -center_embeddings).squeeze(2)  #[batch-size, window_size * 2 * K]\n",
    "#         print(\"shape of pos_dot: {}\".format(pos_dot.shape))\n",
    "#         print(\"shape of neg_dot: {}\".format(neg_dot.shape))\n",
    "        \n",
    "        pos_loss = F.logsigmoid(pos_dot).sum(1)\n",
    "        neg_loss = F.logsigmoid(neg_dot).sum(1)\n",
    "        return - (pos_loss + neg_loss)\n",
    "    \n",
    "    def input_embeddings(self):\n",
    "        return self.in_embedding.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(len(idx_to_word), IN_EMBED_SIZE, OUT_ENBED_SIZE)\n",
    "learning_rate = 4e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter containing:\n",
       "   tensor([[-4.0488e-03, -2.0225e-03,  2.6374e-03,  ..., -1.8155e-03,\n",
       "            -3.1009e-03,  1.0922e-03],\n",
       "           [-3.6676e-03,  2.2173e-03,  3.4155e-03,  ..., -2.1890e-03,\n",
       "             1.0874e-03,  3.0897e-03],\n",
       "           [ 1.0879e-03, -2.6146e-03, -4.3086e-04,  ..., -3.7155e-03,\n",
       "             3.4755e-03, -3.5009e-03],\n",
       "           ...,\n",
       "           [ 3.4506e-03,  4.5707e-03, -4.9445e-03,  ...,  2.0560e-04,\n",
       "            -1.7338e-03,  3.9236e-03],\n",
       "           [-3.4154e-03, -9.6818e-04, -4.0330e-03,  ..., -1.3523e-03,\n",
       "             2.4831e-05,  2.2506e-03],\n",
       "           [ 3.7988e-03, -4.3737e-03, -9.2461e-04,  ..., -3.8285e-03,\n",
       "             3.5673e-03, -3.3511e-03]], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([[ 2.2757e-04, -3.1509e-03, -2.5605e-03,  ...,  4.8006e-03,\n",
       "            -3.3570e-03, -4.6002e-03],\n",
       "           [ 3.2799e-03, -1.2792e-03, -3.6907e-03,  ...,  4.4902e-03,\n",
       "            -2.9896e-03, -3.6164e-03],\n",
       "           [ 2.4741e-03,  3.2170e-03, -2.3588e-03,  ...,  3.6414e-03,\n",
       "            -1.4709e-03,  3.3211e-03],\n",
       "           ...,\n",
       "           [-1.9940e-03, -3.2003e-03,  9.4728e-05,  ...,  1.3046e-03,\n",
       "             5.1640e-04, -3.4807e-03],\n",
       "           [ 2.5612e-03, -1.3423e-03,  2.1049e-03,  ...,  4.1575e-03,\n",
       "            -4.5472e-04, -2.8681e-04],\n",
       "           [-1.8802e-03,  4.7018e-03,  1.5435e-04,  ...,  3.1728e-03,\n",
       "             3.9769e-03, -4.8802e-03]], requires_grad=True)],\n",
       "  'lr': 0.0004,\n",
       "  'betas': (0.9, 0.999),\n",
       "  'eps': 1e-08,\n",
       "  'weight_decay': 0,\n",
       "  'amsgrad': False}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss: 420.0423889160156\n",
      "epoch: 0, iter: 1000, loss: 4.5623908042907715\n",
      "epoch: 0, iter: 2000, loss: 0.971217155456543\n",
      "epoch: 0, iter: 3000, loss: 0.34077128767967224\n",
      "epoch: 0, iter: 4000, loss: 0.16006360948085785\n",
      "epoch: 0, iter: 5000, loss: 0.07626575231552124\n",
      "epoch: 0, iter: 6000, loss: 0.04320208728313446\n",
      "epoch: 0, iter: 7000, loss: 0.02256978675723076\n",
      "epoch: 0, iter: 8000, loss: 0.012630621902644634\n",
      "epoch: 0, iter: 9000, loss: 0.007155262865126133\n",
      "epoch: 0, iter: 10000, loss: 0.003934438340365887\n",
      "epoch: 0, iter: 11000, loss: 0.0023107498418539762\n",
      "epoch: 0, iter: 12000, loss: 0.0013333435636013746\n",
      "epoch: 0, iter: 13000, loss: 0.0007793003460392356\n",
      "epoch: 0, iter: 14000, loss: 0.0004605937283486128\n",
      "epoch: 0, iter: 15000, loss: 0.0002624643675517291\n",
      "epoch: 0, iter: 16000, loss: 0.0001480951759731397\n",
      "epoch: 0, iter: 17000, loss: 9.69022439676337e-05\n",
      "epoch: 0, iter: 18000, loss: 4.51812484243419e-05\n",
      "epoch: 0, iter: 19000, loss: 1.8502583770896308e-05\n",
      "epoch: 0, iter: 20000, loss: 1.0461545571160968e-05\n",
      "epoch: 0, iter: 21000, loss: 7.224268756544916e-06\n",
      "epoch: 0, iter: 22000, loss: 3.340653847772046e-06\n",
      "epoch: 0, iter: 23000, loss: 1.5795229728610138e-06\n",
      "epoch: 0, iter: 24000, loss: 3.855675174690987e-07\n",
      "epoch: 0, iter: 25000, loss: 3.6321576857289983e-08\n",
      "epoch: 0, iter: 26000, loss: 2.7939675018018306e-09\n",
      "epoch: 0, iter: 27000, loss: 0.0\n",
      "epoch: 0, iter: 28000, loss: 0.0\n",
      "epoch: 0, iter: 29000, loss: 0.0\n",
      "epoch: 0, iter: 30000, loss: 0.0\n",
      "epoch: 0, iter: 31000, loss: 0.0\n",
      "epoch: 0, iter: 32000, loss: 0.0\n",
      "epoch: 0, iter: 33000, loss: 0.0\n",
      "epoch: 0, iter: 34000, loss: 0.0\n",
      "epoch: 0, iter: 35000, loss: 0.0\n",
      "epoch: 0, iter: 36000, loss: 0.0\n",
      "epoch: 0, iter: 37000, loss: 0.0\n",
      "epoch: 0, iter: 38000, loss: 0.0\n",
      "epoch: 0, iter: 39000, loss: 0.0\n",
      "epoch: 0, iter: 40000, loss: 0.0\n",
      "epoch: 0, iter: 41000, loss: 0.0\n",
      "epoch: 0, iter: 42000, loss: 0.0\n",
      "epoch: 0, iter: 43000, loss: 0.0\n",
      "epoch: 0, iter: 44000, loss: 0.0\n",
      "epoch: 0, iter: 45000, loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "model = MyModel(len(idx_to_word), IN_EMBED_SIZE, OUT_ENBED_SIZE)\n",
    "learning_rate = 4e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(2):\n",
    "    for iter, (center_word, pos_words, neg_words) in enumerate(dataloader):\n",
    "        loss = model(center_word, pos_words, neg_words).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if iter % 1000 == 0:\n",
    "            print('epoch: {}, iter: {}, loss: {}'.format(epoch, iter, loss))\n",
    "\n",
    "input_embedding = model.input_embeddings()\n",
    "with open('input_embedding', 'w') as fw:\n",
    "    fw.writelines(input_embedding)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('cpmzj': virtualenv)",
   "language": "python",
   "name": "python38364bitcpmzjvirtualenv7f48b8cf2d1441a58ef7e0f71fe74a0f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
