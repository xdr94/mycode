{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE, K = 2, 10\n",
    "VOCAB_SIZE, IN_EMBED_SIZE, OUT_ENBED_SIZE = 2000, 512, 512 \n",
    "BATCH_SIZE = 32\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "\n",
    "vocab_file = 'text8'\n",
    "#prepare vocabulary\n",
    "def tokenize(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "text = None\n",
    "with open(vocab_file) as fr:\n",
    "    text = fr.readlines()[0]\n",
    "    token_list = tokenize(text)\n",
    "vocab =  Counter(token_list).most_common(VOCAB_SIZE - 1)\n",
    "idx_to_word = [item[0] for item in vocab]\n",
    "idx_to_word.append('UNK')\n",
    "word_to_idx = {item: i for i, item in enumerate(idx_to_word)}\n",
    "word_counts = [item[1] for item in vocab]\n",
    "word_counts.append(len(text) - np.sum(word_counts))\n",
    "frequence = word_counts / np.sum(word_counts)\n",
    "frequence = frequence ** (3 / 4)\n",
    "frequence = frequence / np.sum(frequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "class MyDataset(tud.Dataset):\n",
    "    def __init__(self, text, idx_to_word, word_to_idx, WINDOW_SIZE, K, device, frequence):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.text = text\n",
    "        self.idx_to_word = idx_to_word\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.window_size = WINDOW_SIZE\n",
    "        self.k = K\n",
    "        self.device = device\n",
    "        self.frequence = torch.FloatTensor(frequence)\n",
    "        self.word_encode = torch.LongTensor([self.word_to_idx.get(word, self.word_to_idx['UNK']) \\\n",
    "                                             for word in self.text])\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.word_encode)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        center_word = self.word_encode[idx]\n",
    "        pos_index = list(range(idx - self.window_size, idx)) + list(range(idx + 1, idx + self.window_size + 1))\n",
    "        pos_index = [idx % len(self.word_encode) for idx in pos_index]\n",
    "        pos_words = self.word_encode[pos_index]\n",
    "        neg_words = torch.multinomial(self.frequence, self.k * self.window_size * 2, replacement = True)\n",
    "        return center_word, pos_words, neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0\n",
      "torch.Size([32])\n",
      "torch.Size([32, 4])\n",
      "torch.Size([32, 40])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataset = MyDataset(text, idx_to_word, word_to_idx, WINDOW_SIZE, K, device, frequence)  \n",
    "dataloader = tud.DataLoader(dataset, batch_size = BATCH_SIZE,shuffle = True)  \n",
    "for i, (center_word, pos_words, neg_words) in enumerate(dataloader):\n",
    "    print(\"iter: {}\".format(i))\n",
    "    print(center_word.shape)\n",
    "    print(pos_words.shape)\n",
    "    print(neg_words.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, VOCAB_SIZE, IN_EMBED_SIZE, OUT_ENBED_SIZE):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.in_embedding = nn.Embedding(VOCAB_SIZE, IN_EMBED_SIZE)\n",
    "        initrange = 0.5 / IN_EMBED_SIZE\n",
    "        self.in_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out_embedding = nn.Embedding(VOCAB_SIZE, OUT_ENBED_SIZE)\n",
    "        self.out_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, center_word, pos_words, neg_words):\n",
    "        center_embeddings = self.in_embedding(center_word)  #[batch-size, embed_size]\n",
    "        pos_embeddings = self.in_embedding(pos_words)      #[batch-size, window_size * 2, embed_size]\n",
    "        neg_embeddings = self.out_embedding(neg_words)      #[batch-size, window_size * 2 * K, embed_size]\n",
    "        center_embeddings = center_embeddings.unsqueeze(2)                     #[batch-size, embed_size, 1]\n",
    "        \n",
    "        pos_dot = pos_embeddings\n",
    "        \n",
    "        pos_dot = torch.bmm(pos_embeddings, center_embeddings).squeeze(2)  #[batch-size, window_size * 2]\n",
    "        neg_dot = torch.bmm(neg_embeddings, -center_embeddings).squeeze(2)  #[batch-size, window_size * 2 * K]\n",
    "#         print(\"shape of pos_dot: {}\".format(pos_dot.shape))\n",
    "#         print(\"shape of neg_dot: {}\".format(neg_dot.shape))\n",
    "        \n",
    "        pos_loss = F.logsigmoid(pos_dot).sum(1)\n",
    "        neg_loss = F.logsigmoid(neg_dot).sum(1)\n",
    "        return pos_loss + neg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss: -294.1334228515625\n",
      "epoch: 0, iter: 1, loss: -300.57476806640625\n",
      "epoch: 0, iter: 2, loss: -336.6136169433594\n",
      "epoch: 0, iter: 3, loss: -338.8317565917969\n",
      "epoch: 0, iter: 4, loss: -359.3650207519531\n",
      "epoch: 0, iter: 5, loss: -318.126708984375\n",
      "epoch: 0, iter: 6, loss: -350.65216064453125\n",
      "epoch: 0, iter: 7, loss: -332.4310302734375\n",
      "epoch: 0, iter: 8, loss: -393.54791259765625\n",
      "epoch: 0, iter: 9, loss: -295.2746276855469\n",
      "epoch: 0, iter: 10, loss: -292.5108947753906\n",
      "epoch: 0, iter: 11, loss: -339.0655517578125\n",
      "epoch: 0, iter: 12, loss: -314.77655029296875\n",
      "epoch: 0, iter: 13, loss: -303.7771301269531\n",
      "epoch: 0, iter: 14, loss: -328.57135009765625\n",
      "epoch: 0, iter: 15, loss: -359.9826965332031\n",
      "epoch: 0, iter: 16, loss: -314.71478271484375\n",
      "epoch: 0, iter: 17, loss: -325.1954040527344\n",
      "epoch: 0, iter: 18, loss: -389.9200744628906\n",
      "epoch: 0, iter: 19, loss: -335.7273254394531\n",
      "epoch: 0, iter: 20, loss: -400.587158203125\n",
      "epoch: 0, iter: 21, loss: -336.615234375\n",
      "epoch: 0, iter: 22, loss: -332.9488830566406\n",
      "epoch: 0, iter: 23, loss: -398.5390319824219\n",
      "epoch: 0, iter: 24, loss: -316.345947265625\n",
      "epoch: 0, iter: 25, loss: -358.1944885253906\n",
      "epoch: 0, iter: 26, loss: -384.8548889160156\n",
      "epoch: 0, iter: 27, loss: -332.802001953125\n",
      "epoch: 0, iter: 28, loss: -355.41595458984375\n",
      "epoch: 0, iter: 29, loss: -400.86041259765625\n",
      "epoch: 0, iter: 30, loss: -348.699462890625\n",
      "epoch: 0, iter: 31, loss: -458.6483459472656\n",
      "epoch: 0, iter: 32, loss: -374.51953125\n",
      "epoch: 0, iter: 33, loss: -410.8690490722656\n",
      "epoch: 0, iter: 34, loss: -342.11932373046875\n",
      "epoch: 0, iter: 35, loss: -338.2497863769531\n",
      "epoch: 0, iter: 36, loss: -385.96368408203125\n",
      "epoch: 0, iter: 37, loss: -407.4631042480469\n",
      "epoch: 0, iter: 38, loss: -413.0017395019531\n",
      "epoch: 0, iter: 39, loss: -381.23651123046875\n",
      "epoch: 0, iter: 40, loss: -373.71392822265625\n",
      "epoch: 0, iter: 41, loss: -382.0040283203125\n",
      "epoch: 0, iter: 42, loss: -334.35430908203125\n",
      "epoch: 0, iter: 43, loss: -358.0499267578125\n",
      "epoch: 0, iter: 44, loss: -373.4668884277344\n",
      "epoch: 0, iter: 45, loss: -386.13494873046875\n",
      "epoch: 0, iter: 46, loss: -467.69317626953125\n",
      "epoch: 0, iter: 47, loss: -392.3767395019531\n",
      "epoch: 0, iter: 48, loss: -409.81024169921875\n",
      "epoch: 0, iter: 49, loss: -395.118896484375\n",
      "epoch: 0, iter: 50, loss: -447.5435791015625\n",
      "epoch: 0, iter: 51, loss: -400.015380859375\n",
      "epoch: 0, iter: 52, loss: -446.546630859375\n",
      "epoch: 0, iter: 53, loss: -376.1946716308594\n",
      "epoch: 0, iter: 54, loss: -442.2982482910156\n",
      "epoch: 0, iter: 55, loss: -358.9720458984375\n",
      "epoch: 0, iter: 56, loss: -421.69610595703125\n",
      "epoch: 0, iter: 57, loss: -446.21478271484375\n",
      "epoch: 0, iter: 58, loss: -384.1410217285156\n",
      "epoch: 0, iter: 59, loss: -387.32745361328125\n",
      "epoch: 0, iter: 60, loss: -429.8782653808594\n",
      "epoch: 0, iter: 61, loss: -429.8385925292969\n",
      "epoch: 0, iter: 62, loss: -437.1859436035156\n",
      "epoch: 0, iter: 63, loss: -458.09002685546875\n",
      "epoch: 0, iter: 64, loss: -374.17987060546875\n",
      "epoch: 0, iter: 65, loss: -357.7230529785156\n",
      "epoch: 0, iter: 66, loss: -462.37353515625\n",
      "epoch: 0, iter: 67, loss: -448.33197021484375\n",
      "epoch: 0, iter: 68, loss: -426.364990234375\n",
      "epoch: 0, iter: 69, loss: -494.3018493652344\n",
      "epoch: 0, iter: 70, loss: -356.3534240722656\n",
      "epoch: 0, iter: 71, loss: -459.5827941894531\n",
      "epoch: 0, iter: 72, loss: -456.0653991699219\n",
      "epoch: 0, iter: 73, loss: -446.85308837890625\n",
      "epoch: 0, iter: 74, loss: -484.6053161621094\n",
      "epoch: 0, iter: 75, loss: -412.90216064453125\n",
      "epoch: 0, iter: 76, loss: -352.3798828125\n",
      "epoch: 0, iter: 77, loss: -406.91802978515625\n",
      "epoch: 0, iter: 78, loss: -421.193115234375\n",
      "epoch: 0, iter: 79, loss: -385.3928527832031\n",
      "epoch: 0, iter: 80, loss: -486.0407409667969\n",
      "epoch: 0, iter: 81, loss: -408.2740478515625\n",
      "epoch: 0, iter: 82, loss: -464.7838134765625\n",
      "epoch: 0, iter: 83, loss: -444.82275390625\n",
      "epoch: 0, iter: 84, loss: -464.2215881347656\n",
      "epoch: 0, iter: 85, loss: -563.5174560546875\n",
      "epoch: 0, iter: 86, loss: -445.4247741699219\n",
      "epoch: 0, iter: 87, loss: -522.9434204101562\n",
      "epoch: 0, iter: 88, loss: -396.82275390625\n",
      "epoch: 0, iter: 89, loss: -364.1788635253906\n",
      "epoch: 0, iter: 90, loss: -428.11676025390625\n",
      "epoch: 0, iter: 91, loss: -609.2542114257812\n",
      "epoch: 0, iter: 92, loss: -476.3421325683594\n",
      "epoch: 0, iter: 93, loss: -487.52728271484375\n",
      "epoch: 0, iter: 94, loss: -521.7392578125\n",
      "epoch: 0, iter: 95, loss: -431.6462707519531\n",
      "epoch: 0, iter: 96, loss: -462.49554443359375\n",
      "epoch: 0, iter: 97, loss: -478.4893798828125\n",
      "epoch: 0, iter: 98, loss: -487.3171081542969\n",
      "epoch: 0, iter: 99, loss: -507.1846618652344\n",
      "epoch: 0, iter: 100, loss: -548.4827880859375\n",
      "epoch: 0, iter: 101, loss: -530.3614501953125\n",
      "epoch: 0, iter: 102, loss: -487.308837890625\n",
      "epoch: 0, iter: 103, loss: -410.7803649902344\n",
      "epoch: 0, iter: 104, loss: -422.4490661621094\n",
      "epoch: 0, iter: 105, loss: -515.0084838867188\n",
      "epoch: 0, iter: 106, loss: -552.3974609375\n",
      "epoch: 0, iter: 107, loss: -504.5255126953125\n",
      "epoch: 0, iter: 108, loss: -458.6523742675781\n",
      "epoch: 0, iter: 109, loss: -525.5310668945312\n",
      "epoch: 0, iter: 110, loss: -489.79833984375\n",
      "epoch: 0, iter: 111, loss: -571.0106201171875\n",
      "epoch: 0, iter: 112, loss: -499.65594482421875\n",
      "epoch: 0, iter: 113, loss: -579.2272338867188\n",
      "epoch: 0, iter: 114, loss: -499.67974853515625\n",
      "epoch: 0, iter: 115, loss: -529.2612915039062\n",
      "epoch: 0, iter: 116, loss: -501.71392822265625\n",
      "epoch: 0, iter: 117, loss: -471.58746337890625\n",
      "epoch: 0, iter: 118, loss: -434.2889404296875\n",
      "epoch: 0, iter: 119, loss: -530.8580932617188\n",
      "epoch: 0, iter: 120, loss: -514.59130859375\n",
      "epoch: 0, iter: 121, loss: -506.6782531738281\n",
      "epoch: 0, iter: 122, loss: -514.4193725585938\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-0ebd2798ee9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch: {}, iter: {}, loss: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cpmzj/cpmzj/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cpmzj/cpmzj/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train\n",
    "model = MyModel(len(idx_to_word), IN_EMBED_SIZE, OUT_ENBED_SIZE)\n",
    "learning_rate = 4e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for iter, (center_word, pos_words, neg_words) in enumerate(dataloader):\n",
    "        loss = model(center_word, pos_words, neg_words).mean()\n",
    "        print('epoch: {}, iter: {}, loss: {}'.format(epoch, iter, loss))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('cpmzj': virtualenv)",
   "language": "python",
   "name": "python38364bitcpmzjvirtualenv7f48b8cf2d1441a58ef7e0f71fe74a0f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
