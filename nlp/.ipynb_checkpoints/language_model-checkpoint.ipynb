{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as tud\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data\n",
    "BATCH_SIZE = 32\n",
    "sequence_len = 50\n",
    "vocab_size = 30000\n",
    "train_file, dev_file, test_file = [os.path.join('./data', file) \\\n",
    "                                   for file in ['text8.train', 'text8.dev', 'text8.test']]\n",
    "train_raw = open(train_file).readlines()[0]\n",
    "dev_raw = open(dev_file).readlines()[0]\n",
    "test_raw = open(test_file).readlines()[0]\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "vocab = Counter(train_raw.split(' ')).most_common(vocab_size - 1)\n",
    "idx_to_word = [item[0] for item in vocab]\n",
    "idx_to_word.append('UNK')\n",
    "word_to_idx = {word: i for i, word in enumerate(idx_to_word)}\n",
    "\n",
    "class LanguageDataset(tud.Dataset):\n",
    "    def __init__(self, text, sequence_len, idx_to_word, word_to_idx, vocab_size, device):\n",
    "        super(LanguageDataset, self).__init__()\n",
    "        self.device = device\n",
    "        self.vocab_size = vocab_size\n",
    "        self.idx_to_word = idx_to_word\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.word_encode = [self.word_to_idx.get(word, self.vocab_size - 1) for word in text]\n",
    "        self.word_encode = torch.LongTensor(self.word_encode).to(device)\n",
    "        self.sequence_len = sequence_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word_encode) - self.sequence_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.word_encode[idx: min(idx + self.sequence_len, len(self.word_encode) - 1)]\n",
    "        y = self.word_encode[idx + 1: min(idx + self.sequence_len + 1, len(self.word_encode))]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_data = LanguageDataset(train_raw, sequence_len, idx_to_word, word_to_idx, vocab_size, device)\n",
    "dev_data = LanguageDataset(dev_raw, sequence_len, idx_to_word, word_to_idx, vocab_size, device)\n",
    "test_data = LanguageDataset(test_raw, sequence_len, idx_to_word, word_to_idx, vocab_size, device)\n",
    "train_iter = tud.DataLoader(train_data, batch_size = BATCH_SIZE, shuffle = True)\n",
    "dev_iter = tud.DataLoader(dev_data, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_iter = tud.DataLoader(test_data, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50])\n",
      "torch.Size([32, 50])\n",
      "a n d UNK p i c n i c s UNK a c r o s s UNK a r g e n t i n a UNK v e g e t a b l e s UNK a n d UNK s a l a d s\n",
      "n d UNK p i c n i c s UNK a c r o s s UNK a r g e n t i n a UNK v e g e t a b l e s UNK a n d UNK s a l a d s UNK\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(train_iter):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(' '.join([idx_to_word[idx] for idx in x[0]]))\n",
    "    print(' '.join([idx_to_word[idx] for idx in y[0]]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "import torch\n",
    "embed_size, hidden_size = 300, 1000\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        input_x = self.embed(x)     #batch_size * sequence_len * embedding_size\n",
    "        output, hidden = self.lstm(input_x, hidden)  #output: batch_size * sequence_len * embedding_size\n",
    "        output_vocab = self.linear(output)   #output_vocab:  batch_size * sequence_len * vocab_size\n",
    "        return output_vocab, hidden\n",
    "    \n",
    "    def init_hidden(self, bsz, requires_grad=True):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros((1, bsz, self.hidden_size), requires_grad=requires_grad),\n",
    "                    weight.new_zeros((1, bsz, self.hidden_size), requires_grad=requires_grad))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_iter0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-236-09ca4a36a39a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mdev_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: {}, iter: {}, train loss: {}, dev loss: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_loss_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdev_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_loss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-236-09ca4a36a39a>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, data_iter)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepackage_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_iter0' is not defined"
     ]
    }
   ],
   "source": [
    "#train\n",
    "model = LanguageModel(vocab_size, embed_size, hidden_size).to(device)\n",
    "learning_rate = 4e-4\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)\n",
    "requires_grad = False\n",
    "GRAD_CLIP = 1.0\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "def evaluate(model, data_iter):\n",
    "    model.eval()\n",
    "    loss_all = 0.\n",
    "    count = 0.\n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden(BATCH_SIZE, requires_grad=False)\n",
    "        print(len(data_iter))\n",
    "        for i, (x, y) in enumerate(data_iter):\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output, hidden = model(x, hidden)\n",
    "            loss = loss_fn(output.view(-1, vocab_size), y.view(-1))\n",
    "            loss_all += loss * x.shape[0]\n",
    "            count += x.shape[0]\n",
    "    mode.train()\n",
    "    return loss_all / count\n",
    "    \n",
    "dev_loss_list = []\n",
    "model_path = './best_mode.pth'\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    hidden = model.init_hidden(BATCH_SIZE)\n",
    "    for i, (x, y) in enumerate(train_iter):\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(x, hidden)\n",
    "        loss = loss_fn(output.view(-1, vocab_size), y.view(-1))\n",
    "        loss.requires_grad_(True)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch: {}, iter: {}, train loss: {}\".format(epoch, i, loss))\n",
    "            \n",
    "        if i % 1000 == 0:\n",
    "            dev_loss = evaluate(model, dev_iter)\n",
    "            print(\"Epoch: {}, iter: {}, dev loss: {}\".format(epoch, i, dev_loss))\n",
    "            if len(dev_loss_list) == 0 or dev_loss < min(dev_loss_list):\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            dev_loss_list.append(dev_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = LanguageModel(vocab_size, embed_size, hidden_size)\n",
    "test_model.load_state_dict(torch.load(model_path))\n",
    "words_list = []\n",
    "input_x = torch.randint(vocab_size, (1, 1), dtype=torch.long)\n",
    "hidden = test_model.init_hidden(1)\n",
    "for i in range(100):\n",
    "    output, hidden = model(input_x, hidden)\n",
    "    y = torch.argmax(output.view(-1))\n",
    "    input_x.fill_(y)\n",
    "    word = idx_to_word[y]\n",
    "    words_list.append(word)\n",
    "print(' '.join(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "rnn = nn.LSTM(10, 20, 2) #embedding_size, hidden_size, num_layer\n",
    "input = torch.randn(5, 3, 10)   #sequence_len, batch_size, embedding_size\n",
    "h0 = torch.randn(2, 3, 20)   #num_layer, batch_size, hidden_size\n",
    "c0 = torch.randn(2, 3, 20)   #num_layer, batch_size, hidden_size\n",
    "output, (hn, cn) = rnn(input, (h0, c0))\n",
    "#output: sequence_len, batch_size, embedding_size\n",
    "weights = next(rnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
